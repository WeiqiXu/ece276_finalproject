{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld_env import gridWorld_env\n",
    "from cursor1D_env import cursor1D_env\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Method Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q saved is a list of Q matrices\n",
    "#Each Q at index i is the Q estimate at episode number i\n",
    "\n",
    "def runQLearning(env, learning_rate,discount_factor, num_of_episodes, num_steps_per_episode, Q0, l, explore_type='linear', explore_const = 0.2):\n",
    "    #%% code starts here\n",
    "    \n",
    "    Q_saved = [Q0.copy()]\n",
    "    Q = Q0.copy()\n",
    "    \n",
    "    for episode in range(num_of_episodes):\n",
    "        #Start new episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        exploration = 0\n",
    "        \n",
    "        #Exploration rate:\n",
    "        if explore_type == 'linear':\n",
    "            exploration = 1 - episode/num_of_episodes\n",
    "        elif explore_type == 'logarithmic':\n",
    "            exploration = 1000.0/(1000.0 + episode)\n",
    "        elif explore_type == 'constant':\n",
    "                exploration = explore_const\n",
    "        else:\n",
    "            print(\"Invalid exploration type.\")\n",
    "            return\n",
    "        \n",
    "        for _ in range(num_steps_per_episode):\n",
    "            \n",
    "            #Perform an epsilon greedy action\n",
    "            action = 0;\n",
    "            r = np.random.rand()\n",
    "            if r > exploration:\n",
    "                action = np.argmax(Q0[state, :])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            nextState, reward, done, _ = env.step(action, l)\n",
    "            \n",
    "            #Update Q\n",
    "            Q[state, action] = Q[state, action] + learning_rate*(reward + discount_factor*np.max(Q[nextState, :]) - Q[state, action])\n",
    "            \n",
    "            state = nextState;\n",
    "    \n",
    "        #Store Q into Q_saved\n",
    "        Q_saved.append(Q.copy())\n",
    "        \n",
    "    return Q_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tests the policy. Gives number of steps to reach goal and if goal is reached\n",
    "\n",
    "def testPolicy(env, policy, max_num_steps = 30, render = False):\n",
    "    s = env.reset()\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    num_steps = 0\n",
    "    \n",
    "    for _ in range(max_num_steps):\n",
    "        \n",
    "        a = int(policy[s])\n",
    "        s, r, d, _ = env.step(a, l)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        num_steps += 1\n",
    "        \n",
    "        if d == True:\n",
    "            break\n",
    "            \n",
    "    return num_steps, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets baseline results for gridworld env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is baseline performance of our solution\n",
    "\n",
    "num_s = 16\n",
    "num_a = 4\n",
    "\n",
    "optimal_number_steps = 6\n",
    "\n",
    "num_participants = 100\n",
    "num_steps_per_participant = 100\n",
    "\n",
    "#We are going to test where truePositiveRate = 1 - falsePositiveRate\n",
    "rate = np.arange(0.50, 1.01, 0.05)\n",
    "num_times_to_run = 1000\n",
    "\n",
    "mean_num_participants_to_converge = []\n",
    "std_num_participants_to_converge = []\n",
    "\n",
    "mean_num_participants_to_solve = []\n",
    "std_num_participants_to_solve = []\n",
    "\n",
    "for r in rate:\n",
    "    env = gridWorld_env(truePositiveRate = r, falsePositiveRate = 1-r)\n",
    "\n",
    "    #We want to find statistics for number of participants required to get optimal_number_steps\n",
    "    #And number of participants needed to simply solve the environment\n",
    "    num_participants_to_converge_list = [];\n",
    "    num_participants_to_solve_list = []\n",
    "\n",
    "    for _ in range(num_times_to_run):\n",
    "        #Run Q learning\n",
    "        Q0 = np.zeros((num_s, num_a))\n",
    "        Q_saved = runQLearning(env, 0.02,0.95, num_participants, num_steps_per_participant, Q0, 1,  explore_type = 'constant', explore_const = 1.0)\n",
    "\n",
    "        steps_all = []\n",
    "        done_all = []\n",
    "\n",
    "        policy = np.zeros((num_s,))\n",
    "        \n",
    "        \n",
    "        #Get the sucess rates\n",
    "        for i in range(len(Q_saved)):\n",
    "            Q = Q_saved[i]    \n",
    "\n",
    "            #Get the policy from that Q\n",
    "            for s in range(num_s):\n",
    "                policy[s] = np.argmax(Q[s,:])\n",
    "                \n",
    "\n",
    "            steps, done = testPolicy(env, policy)\n",
    "\n",
    "            steps_all.append(steps)\n",
    "            done_all.append(done)\n",
    "        \n",
    "        \n",
    "        #Check how many participants were needed to converge\n",
    "        # There is a plus 1 here because the first Q in Q_saved is from Q0 (so 0 participants)\n",
    "        #Also so when it breaks from the loop searching, it will mean we needed the next participant to converge.\n",
    "        num_participants_to_converge = num_participants + 1\n",
    "        for step in steps_all[::-1]:\n",
    "            if step != optimal_number_steps:\n",
    "                break\n",
    "            num_participants_to_converge -= 1\n",
    "\n",
    "        num_participants_to_solve = num_participants + 1\n",
    "        for done in done_all[::-1]:\n",
    "            if not done:\n",
    "                break\n",
    "            num_participants_to_solve -= 1\n",
    "        \n",
    "        num_participants_to_converge_list.append(num_participants_to_converge)\n",
    "        num_participants_to_solve_list.append(num_participants_to_solve)\n",
    "\n",
    "\n",
    "    num_participants_to_converge_list = np.array(num_participants_to_converge_list)\n",
    "        \n",
    "    mean_num_participants_to_converge.append(np.mean(num_participants_to_converge_list))\n",
    "    std_num_participants_to_converge.append(np.std(num_participants_to_converge_list))\n",
    "    \n",
    "    num_participants_to_solve_list = np.array(num_participants_to_solve_list)\n",
    "    \n",
    "    mean_num_participants_to_solve.append(np.mean(num_participants_to_solve_list))\n",
    "    std_num_participants_to_solve.append(np.std(num_participants_to_solve_list))\n",
    "\n",
    "    \n",
    "    print(\"r = \" + str(r))\n",
    "    print(\"mean to converge = \" + str(mean_num_participants_to_converge[-1]))\n",
    "    print(\"mean to solve    = \" + str(mean_num_participants_to_solve[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_num_participants_to_converge = np.array(mean_num_participants_to_converge)\n",
    "std_num_participants_to_converge = np.array(std_num_participants_to_converge)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.errorbar(rate, mean_num_participants_to_converge, std_num_participants_to_converge, fmt='-og', lw=1)\n",
    "\n",
    "plt.title('Number of Participants Required to Find Optimal Solution')\n",
    "plt.xlabel('Rate of error')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)\n",
    "\n",
    "mean_num_participants_to_solve = np.array(mean_num_participants_to_solve)\n",
    "std_num_participants_to_solve = np.array(std_num_participants_to_solve)\n",
    "max_num_participants_to_solve = np.array(max_num_participants_to_solve)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.errorbar(rate, mean_num_participants_to_solve, std_num_participants_to_solve, fmt='-og', lw=1)\n",
    "\n",
    "plt.title('Number of Participants Required to Find A Solution')\n",
    "plt.xlabel('Rate of error')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hueristic reward for grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass to overwrite the generalized reward:\n",
    "\n",
    "class gridWorld_env1(gridWorld_env):\n",
    "    \n",
    "    def __init__(self, truePositiveRate = 0.7, falsePositiveRate = 0.3):\n",
    "        super(gridWorld_env1,self).__init__(truePositiveRate, falsePositiveRate)\n",
    "        \n",
    "        \n",
    "        # a = 0 is left, a = 1 is down, a = 2 is right, a = 3 is up\n",
    "        #Below defines the hueristic reward. 1 is where we give reward, 0 is not\n",
    "        R = [[0, 1, 1, 0], [0, 1, 1, 0], [0, 1, 1, 0], [1, 1, 1, 0],\n",
    "             [0, 1, 1, 0], [0, 1, 1, 0], [0, 1, 1, 0], [1, 1, 1, 0],\n",
    "             [0, 1, 1, 0], [0, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 1],\n",
    "             [0, 1, 1, 1], [0, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]\n",
    "        \n",
    "        self.R = np.array(R)\n",
    "        \n",
    "        self.k = self.falsePositiveRate*(self.R.size/np.count_nonzero(self.R))\n",
    "        \n",
    "    def getHeuristicReward(self, s,  a, last_s):\n",
    "        if self.R[s, a]:\n",
    "            return self.k\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hueristic Reward results for grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This is baseline performance of our solution\n",
    "\n",
    "num_s = 16\n",
    "num_a = 4\n",
    "\n",
    "optimal_number_steps = 6\n",
    "num_participants = 100\n",
    "num_steps_per_participant = 100\n",
    "\n",
    "num_trial_to_train_initial = 1000\n",
    "\n",
    "\n",
    "#We are going to test where truePositiveRate = 1 - falsePositiveRate\n",
    "rate = np.arange(0.50, 1.01, 0.05)\n",
    "num_times_to_run = 1000\n",
    "\n",
    "num_participants_to_solve_list_ours = []\n",
    "num_participants_to_converge_list_ours = []\n",
    "\n",
    "mean_num_participants_to_converge_ours = []\n",
    "std_num_participants_to_converge_ours = []\n",
    "\n",
    "mean_num_participants_to_solve_ours = []\n",
    "std_num_participants_to_solve_ours = []\n",
    "\n",
    "for r in rate:\n",
    "    env = gridWorld_env1(truePositiveRate = r, falsePositiveRate = 1-r)\n",
    "        \n",
    "    \n",
    "    #We want to find statistics for number of participants required to get optimal_number_steps\n",
    "    #And number of participants needed to simply solve the environment\n",
    "    num_participants_to_converge_list = [];\n",
    "    num_participants_to_solve_list = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_times_to_run):\n",
    "        \n",
    "        #Going to train initial Q0 here\n",
    "        Q0 = np.zeros((num_s, num_a))\n",
    "        for _ in range(num_trial_to_train_initial) :\n",
    "            Q_init = runQLearning(env, 0.02 ,0.95, 1, num_steps_per_participant, Q0, 0,  explore_type = 'constant', explore_const = 1.0)\n",
    "            Q0 = Q_init[-1]  \n",
    "\n",
    "                    \n",
    "        #Run Q learning\n",
    "        Q_saved = runQLearning(env, 0.02,0.95, num_participants, num_steps_per_participant, Q0, 1,  explore_type = 'constant', explore_const = r)\n",
    "        \n",
    "        \n",
    "        steps_all = []\n",
    "        done_all = []\n",
    "\n",
    "        policy = np.zeros((num_s,))\n",
    "        \n",
    "        \n",
    "        #Get the sucess rates\n",
    "        for i in range(len(Q_saved)):\n",
    "            Q = Q_saved[i]    \n",
    "\n",
    "            #Get the policy from that Q\n",
    "            for s in range(num_s):\n",
    "                policy[s] = np.argmax(Q[s,:])\n",
    "                \n",
    "\n",
    "            steps, done = testPolicy(env, policy)\n",
    "\n",
    "            steps_all.append(steps)\n",
    "            done_all.append(done)\n",
    "        \n",
    "        \n",
    "        #Check how many participants were needed to converge\n",
    "        # There is a plus 1 here because the first Q in Q_saved is from Q0 (so 0 participants)\n",
    "        #Also so when it breaks from the loop searching, it will mean we needed the next participant to converge.\n",
    "        num_participants_to_converge = num_participants + 1\n",
    "        for step in steps_all[::-1]:\n",
    "            if step != optimal_number_steps:\n",
    "                break\n",
    "            num_participants_to_converge -= 1\n",
    "\n",
    "        num_participants_to_solve = num_participants + 1\n",
    "        for done in done_all[::-1]:\n",
    "            if not done:\n",
    "                break\n",
    "            num_participants_to_solve -= 1\n",
    "        \n",
    "        num_participants_to_converge_list.append(num_participants_to_converge)\n",
    "        num_participants_to_solve_list.append(num_participants_to_solve)\n",
    "\n",
    "\n",
    "    num_participants_to_converge_list_ours = np.array(num_participants_to_converge_list)\n",
    "        \n",
    "    mean_num_participants_to_converge_ours.append(np.mean(num_participants_to_converge_list))\n",
    "    std_num_participants_to_converge_ours.append(np.std(num_participants_to_converge_list))\n",
    "    \n",
    "    num_participants_to_solve_list_ours = np.array(num_participants_to_solve_list)\n",
    "    \n",
    "    mean_num_participants_to_solve_ours.append(np.mean(num_participants_to_solve_list))\n",
    "    std_num_participants_to_solve_ours.append(np.std(num_participants_to_solve_list))\n",
    "\n",
    "    \n",
    "    print(\"r = \" + str(r))\n",
    "    print(\"mean to converge = \" + str(num_participants_to_converge_list_ours[-1]))\n",
    "    print(\"mean to solve    = \" + str(num_participants_to_solve_list_ours[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline\n",
    "\n",
    "mean_num_participants_to_converge = np.array(mean_num_participants_to_converge)\n",
    "std_num_participants_to_converge = np.array(std_num_participants_to_converge)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.errorbar(rate, mean_num_participants_to_converge, std_num_participants_to_converge, fmt='-og', lw=1)\n",
    "\n",
    "plt.title('Number of Participants Required to Find Optimal Solution')\n",
    "plt.xlabel('Rate of error')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)\n",
    "\n",
    "mean_num_participants_to_solve = np.array(mean_num_participants_to_solve)\n",
    "std_num_participants_to_solve = np.array(std_num_participants_to_solve)\n",
    "max_num_participants_to_solve = np.array(max_num_participants_to_solve)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.errorbar(rate, mean_num_participants_to_solve, std_num_participants_to_solve, fmt='-og', lw=1)\n",
    "\n",
    "plt.title('Number of Participants Required to Find A Solution')\n",
    "plt.xlabel('Rate of error')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)\n",
    "\n",
    "#Plot our solution\n",
    "\n",
    "mean_num_participants_to_converge = np.array(mean_num_participants_to_converge)\n",
    "std_num_participants_to_converge = np.array(std_num_participants_to_converge)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.errorbar(rate, mean_num_participants_to_converge, std_num_participants_to_converge, fmt='-ob', lw=1, label='Baseline')\n",
    "\n",
    "mean_num_participants_to_solve = np.array(mean_num_participants_to_solve)\n",
    "std_num_participants_to_solve = np.array(std_num_participants_to_solve)\n",
    "max_num_participants_to_solve = np.array(max_num_participants_to_solve)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.errorbar(rate, mean_num_participants_to_solve, std_num_participants_to_solve, fmt='-ob', lw=1, label='Baseline')\n",
    "\n",
    "\n",
    "mean_num_participants_to_converge_ours = np.array(mean_num_participants_to_converge_ours)\n",
    "std_num_participants_to_converge_ours = np.array(std_num_participants_to_converge_ours)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.errorbar(rate, mean_num_participants_to_converge_ours, std_num_participants_to_converge_ours, fmt='-og', lw=1, label='Using Heuristic')\n",
    "plt.legend()\n",
    "plt.title('Number of Participants Required to Find Optimal Solution')\n",
    "plt.xlabel('p1 = 1 - p0')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)\n",
    "\n",
    "mean_num_participants_to_solve_ours = np.array(mean_num_participants_to_solve_ours)\n",
    "std_num_participants_to_solve_ours = np.array(std_num_participants_to_solve_ours)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.errorbar(rate, mean_num_participants_to_solve_ours, std_num_participants_to_solve_ours, fmt='-og', lw=1, label='Using Heuristic')\n",
    "plt.legend()\n",
    "plt.title('Number of Participants Required to Find Optimal Solution')\n",
    "plt.xlabel('p1 = 1 - p0')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "# Define your network here\n",
    "\n",
    "    def __init__(self,  state_size, action_size, hidden_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        #Set up the network\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "                \n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Replay():\n",
    "# Replay should also have an initialize method which creates a minimum buffer for \n",
    "# the initial episodes to generate minibatches.\n",
    "\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.size    = max_size\n",
    "        self.buffer  = []\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def pushBack(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        #if the list isn't at it's maximum capacity\n",
    "        if(len(self.buffer) < self.size):\n",
    "            self.buffer.append([state, action, reward, next_state, done])\n",
    "            \n",
    "        #Otherwise overwrite\n",
    "        else:\n",
    "            self.buffer[self.pointer] = [state, action, reward, next_state, done]\n",
    "        \n",
    "        #Update pointer\n",
    "        self.pointer = (self.pointer + 1)%self.size\n",
    "        \n",
    "    def getRandom(self, batch_size):        \n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def getRandomSeperated(self, batch_size):\n",
    "        batch = self.getRandom(batch_size)\n",
    "        states      = []\n",
    "        actions     = []\n",
    "        rewards     = []\n",
    "        next_states = []\n",
    "        dones       = []\n",
    "\n",
    "        for el in batch:\n",
    "            states.append(el[0]); actions.append(el[1]); rewards.append(el[2]); next_states.append(el[3]); dones.append(el[4])\n",
    "            \n",
    "        return FloatTensor(states), LongTensor(actions), FloatTensor(rewards), FloatTensor(next_states), dones\n",
    "    \n",
    "    def initialize(self, env, init_length=1000):\n",
    "        state = env.reset()\n",
    "        \n",
    "        for _ in range(init_length):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            self.pushBack(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "            if done :\n",
    "                state = env.reset()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to evaluate DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateDQN(env, DQN, render = False):\n",
    "    state = env.reset_left()\n",
    "    passedLeft = False\n",
    "    \n",
    "    for step in range(1000):\n",
    "        #Get action from DQN   \n",
    "        dumb_package_pytorch = FloatTensor(np.array([state]))\n",
    "        _ , action = torch.max(DQN(dumb_package_pytorch), 0)\n",
    "        if use_cuda:\n",
    "            action_to_take = action.cpu().numpy\n",
    "        else:\n",
    "            action_to_take = action.numpy\n",
    "        \n",
    "        #Perform action and render\n",
    "        next_state, reward, done, _ = env.step(action_to_take, 1)\n",
    "        if render:\n",
    "            env.render(display_sr = True, display_hm = True)\n",
    "        state = next_state\n",
    "        \n",
    "        if abs(env.goal_location - state) < 0.025:\n",
    "            passedLeft = True\n",
    "        else:\n",
    "            passedLeft = False\n",
    "        \n",
    "    state = env.reset_right()\n",
    "    for step in range(1000):\n",
    "\n",
    "        #Get action from DQN     \n",
    "        dumb_package_pytorch = FloatTensor(np.array([state]))\n",
    "        _ , action = torch.max(DQN(dumb_package_pytorch), 0)\n",
    "        if use_cuda:\n",
    "            action_to_take = action.cpu().numpy\n",
    "        else:\n",
    "            action_to_take = action.numpy\n",
    "\n",
    "        #Perform action and render\n",
    "        next_state, reward, done, _ = env.step(action_to_take, 1)\n",
    "        if render:\n",
    "            env.render(display_sr = True, display_hm = True)\n",
    "        state = next_state\n",
    "        \n",
    "        if abs(env.goal_location - state) < 0.025:\n",
    "            passedRight = True\n",
    "        else:\n",
    "            passedRight = False\n",
    "        \n",
    "        \n",
    "    return passedLeft and passedRight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Baseline DQN performance in cursor env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On participant 1\n",
      "Training DQN\n",
      "Converged = False\n",
      "On participant 2\n",
      "Training DQN\n",
      "Converged = False\n",
      "On participant 3\n",
      "Training DQN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0c0e44971ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mQt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mnot_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mQt\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnot_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_participants = 100\n",
    "\n",
    "num_trails_per_participants = 10 #Roughly 5 minutes per participant\n",
    "num_steps_per_trail = 2000 #This is roughly 30 seconds per trial\n",
    "num_steps_till_next_action = 100 #Â A new action is taken roughly every 1.5 seconds\n",
    "\n",
    "num_it_to_update_DQN = 10000\n",
    "\n",
    "gamma = 0.98\n",
    "epsilon = 1\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_size = 20\n",
    "\n",
    "state_size = 1\n",
    "action_size = 3\n",
    "\n",
    "env = cursor1D_env(truePositiveRate = 1, falsePositiveRate = 0)\n",
    "\n",
    "## Initialize Replay Buffer\n",
    "replay = Replay(max_size=num_participants*num_trails_per_participants*num_steps_per_trail)\n",
    "\n",
    "#Data collection!\n",
    "for participant in range(1,num_participants+1):\n",
    "\n",
    "    print(\"On participant {}\".format(participant))\n",
    "    \n",
    "    # --> start participant \n",
    "    for _ in range(num_trails_per_participants):\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        num_steps_in_episode = 0\n",
    "        \n",
    "        for step in range(num_steps_per_trail):\n",
    "            # explore/exploit and get action using DQN\n",
    "            r = np.random.rand()\n",
    "\n",
    "            if step % num_steps_till_next_action == 0:\n",
    "                if r > epsilon:\n",
    "                    dumb_package_pytorch = FloatTensor(torch.from_numpy(state))\n",
    "\n",
    "                    _ , action = torch.max(DQN0(dumb_package_pytorch), 0)\n",
    "                    action_to_take = action.data[0]\n",
    "\n",
    "                else:\n",
    "                    action_to_take = env.random_action()\n",
    "                    \n",
    "            # perform action and record new_state, action, reward\n",
    "            next_state, reward, done, _ = env.step(action_to_take, 1)\n",
    "\n",
    "            # populate Replay experience buffer\n",
    "            replay.pushBack(state, action_to_take, reward, next_state, done)\n",
    "\n",
    "            #update state with next state and apply \n",
    "            state = next_state\n",
    "            total_reward = reward + total_reward\n",
    "            num_steps_in_episode = num_steps_in_episode + 1\n",
    "            \n",
    "    # <-- end participant\n",
    "    \n",
    "    print(\"Training DQN\")\n",
    "    \n",
    "    \n",
    "    if use_cuda:\n",
    "        DQN = QNetwork(state_size, action_size, hidden_size).cuda()\n",
    "        targetDQN = QNetwork(state_size, action_size, hidden_size).cuda()\n",
    "    else:\n",
    "        DQN = QNetwork(state_size, action_size, hidden_size)\n",
    "        targetDQN = QNetwork(state_size, action_size, hidden_size)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(DQN.parameters(), learning_rate)\n",
    "\n",
    "    # set targetDQN weights to DQN weights\n",
    "    targetDQN.load_state_dict(DQN.state_dict())\n",
    "    \n",
    "    #Now time to update the DQN with all the data in replay buffer a few times\n",
    "    for _ in range(num_it_to_update_DQN):\n",
    "        \n",
    "        # Get batch from replay\n",
    "        states, actions, rewards, next_states, done = replay.getRandomSeperated(batch_size)\n",
    "\n",
    "        \n",
    "        # update DQN (run one epoch of training per episode with generated minibatch of states and qvalues)\n",
    "        Qt, _ = torch.max(targetDQN(torch.unsqueeze(Variable(next_states, requires_grad = False), -1)), 1)\n",
    "\n",
    "        not_done = FloatTensor(np.invert(np.array(done)).astype(int)).reshape(batch_size)\n",
    "        \n",
    "        Qt =  torch.mul(Qt,not_done)\n",
    "\n",
    "        QTargets = gamma*Qt + Variable(rewards)\n",
    "        QBeh  = DQN(torch.unsqueeze(Variable(states), -1)).gather(1, Variable(actions.view(-1,1)))\n",
    "        \n",
    "       \n",
    "        QTargets = QTargets.detach()\n",
    "        QTargets = QTargets.reshape(QBeh.shape)\n",
    "                \n",
    "        #Compute the loss\n",
    "        loss = criterion(QBeh, QTargets)\n",
    "\n",
    "        #Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # set targetDQN weights to DQN weights\n",
    "        targetDQN.load_state_dict(DQN.state_dict())\n",
    "    \n",
    "    if participant % 5 == 0:    \n",
    "        print(\"Rendering DQN perfomance\")\n",
    "        #Render the current DQN performance:\n",
    "        passed = evaluateDQN(env, DQN, render=False)\n",
    "    else:\n",
    "        passed = evaluateDQN(env, DQN, render=False)\n",
    "        \n",
    "    print(\"Converged = {}\".format(passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset_left()\n",
    "for _ in range(1000):\n",
    "\n",
    "    #Get action from DQN        \n",
    "    dumb_package_pytorch = Variable(torch.from_numpy(np.array([state]))).float()\n",
    "    _ , action = torch.max(DQN(dumb_package_pytorch), 0)\n",
    "    action_to_take = action.data[0]\n",
    "\n",
    "    #Perform action and render\n",
    "    next_state, reward, done, _ = env.step(action_to_take, 1)\n",
    "    env.render(display_sr = True)\n",
    "    state = next_state\n",
    "\n",
    "state = env.reset_right()\n",
    "for _ in range(1000):\n",
    "\n",
    "    #Get action from DQN        \n",
    "    dumb_package_pytorch = Variable(torch.from_numpy(np.array([state]))).float()\n",
    "    _ , action = torch.max(DQN(dumb_package_pytorch), 0)\n",
    "    action_to_take = action.data[0]\n",
    "\n",
    "    #Perform action and render\n",
    "    next_state, reward, done, _ = env.step(action_to_take, 1)\n",
    "    env.render(display_sr = True)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ece276c",
   "language": "python",
   "name": "ece276c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
