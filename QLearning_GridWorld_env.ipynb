{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld_env import gridWorld_env\n",
    "from cursor1D_env import cursor1D_env\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Method Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q saved is a list of Q matrices\n",
    "#Each Q at index i is the Q estimate at episode number i\n",
    "\n",
    "def runQLearning(env, learning_rate,discount_factor, num_of_episodes, num_steps_per_episode, Q0, l, explore_type='linear', explore_const = 0.2):\n",
    "    #%% code starts here\n",
    "    \n",
    "    Q_saved = [Q0.copy()]\n",
    "    Q = Q0.copy()\n",
    "    \n",
    "    for episode in range(num_of_episodes):\n",
    "        #Start new episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        exploration = 0\n",
    "        \n",
    "        #Exploration rate:\n",
    "        if explore_type == 'linear':\n",
    "            exploration = 1 - episode/num_of_episodes\n",
    "        elif explore_type == 'logarithmic':\n",
    "            exploration = 1000.0/(1000.0 + episode)\n",
    "        elif explore_type == 'constant':\n",
    "                exploration = explore_const\n",
    "        else:\n",
    "            print(\"Invalid exploration type.\")\n",
    "            return\n",
    "        \n",
    "        for _ in range(num_steps_per_episode):\n",
    "            \n",
    "            #Perform an epsilon greedy action\n",
    "            action = 0;\n",
    "            r = np.random.rand()\n",
    "            if r > exploration:\n",
    "                action = np.argmax(Q0[state, :])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            nextState, reward, done, _ = env.step(action, l)\n",
    "            \n",
    "            #Update Q\n",
    "            Q[state, action] = Q[state, action] + learning_rate*(reward + discount_factor*np.max(Q[nextState, :]) - Q[state, action])\n",
    "            \n",
    "            state = nextState;\n",
    "    \n",
    "        #Store Q into Q_saved\n",
    "        Q_saved.append(Q.copy())\n",
    "        \n",
    "    return Q_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tests the policy. Gives number of steps to reach goal and if goal is reached\n",
    "\n",
    "def testPolicy(env, policy, max_num_steps = 30, render = False):\n",
    "    s = env.reset()\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    num_steps = 0\n",
    "    \n",
    "    for _ in range(max_num_steps):\n",
    "        \n",
    "        a = int(policy[s])\n",
    "        s, r, d, _ = env.step(a, l)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        num_steps += 1\n",
    "        \n",
    "        if d == True:\n",
    "            break\n",
    "            \n",
    "    return num_steps, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets baseline results for gridworld env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is baseline performance of our solution\n",
    "\n",
    "num_s = 16\n",
    "num_a = 4\n",
    "\n",
    "optimal_number_steps = 6\n",
    "\n",
    "num_participants = 100\n",
    "num_steps_per_participant = 100\n",
    "\n",
    "#We are going to test where truePositiveRate = 1 - falsePositiveRate\n",
    "rate = np.arange(0.50, 1.01, 0.05)\n",
    "num_times_to_run = 1000\n",
    "\n",
    "mean_num_participants_to_converge = []\n",
    "std_num_participants_to_converge = []\n",
    "\n",
    "mean_num_participants_to_solve = []\n",
    "std_num_participants_to_solve = []\n",
    "\n",
    "for r in rate:\n",
    "    env = gridWorld_env(truePositiveRate = r, falsePositiveRate = 1-r)\n",
    "\n",
    "    #We want to find statistics for number of participants required to get optimal_number_steps\n",
    "    #And number of participants needed to simply solve the environment\n",
    "    num_participants_to_converge_list = [];\n",
    "    num_participants_to_solve_list = []\n",
    "\n",
    "    for _ in range(num_times_to_run):\n",
    "        #Run Q learning\n",
    "        Q0 = np.zeros((num_s, num_a))\n",
    "        Q_saved = runQLearning(env, 0.02,0.95, num_participants, num_steps_per_participant, Q0, 1,  explore_type = 'constant', explore_const = 1.0)\n",
    "\n",
    "        steps_all = []\n",
    "        done_all = []\n",
    "\n",
    "        policy = np.zeros((num_s,))\n",
    "        \n",
    "        \n",
    "        #Get the sucess rates\n",
    "        for i in range(len(Q_saved)):\n",
    "            Q = Q_saved[i]    \n",
    "\n",
    "            #Get the policy from that Q\n",
    "            for s in range(num_s):\n",
    "                policy[s] = np.argmax(Q[s,:])\n",
    "                \n",
    "\n",
    "            steps, done = testPolicy(env, policy)\n",
    "\n",
    "            steps_all.append(steps)\n",
    "            done_all.append(done)\n",
    "        \n",
    "        \n",
    "        #Check how many participants were needed to converge\n",
    "        # There is a plus 1 here because the first Q in Q_saved is from Q0 (so 0 participants)\n",
    "        #Also so when it breaks from the loop searching, it will mean we needed the next participant to converge.\n",
    "        num_participants_to_converge = num_participants + 1\n",
    "        for step in steps_all[::-1]:\n",
    "            if step != optimal_number_steps:\n",
    "                break\n",
    "            num_participants_to_converge -= 1\n",
    "\n",
    "        num_participants_to_solve = num_participants + 1\n",
    "        for done in done_all[::-1]:\n",
    "            if not done:\n",
    "                break\n",
    "            num_participants_to_solve -= 1\n",
    "        \n",
    "        num_participants_to_converge_list.append(num_participants_to_converge)\n",
    "        num_participants_to_solve_list.append(num_participants_to_solve)\n",
    "\n",
    "\n",
    "    num_participants_to_converge_list = np.array(num_participants_to_converge_list)\n",
    "        \n",
    "    mean_num_participants_to_converge.append(np.mean(num_participants_to_converge_list))\n",
    "    std_num_participants_to_converge.append(np.std(num_participants_to_converge_list))\n",
    "    \n",
    "    num_participants_to_solve_list = np.array(num_participants_to_solve_list)\n",
    "    \n",
    "    mean_num_participants_to_solve.append(np.mean(num_participants_to_solve_list))\n",
    "    std_num_participants_to_solve.append(np.std(num_participants_to_solve_list))\n",
    "\n",
    "    \n",
    "    print(\"r = \" + str(r))\n",
    "    print(\"mean to converge = \" + str(mean_num_participants_to_converge[-1]))\n",
    "    print(\"mean to solve    = \" + str(mean_num_participants_to_solve[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_num_participants_to_converge = np.array(mean_num_participants_to_converge)\n",
    "std_num_participants_to_converge = np.array(std_num_participants_to_converge)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.errorbar(rate, mean_num_participants_to_converge, std_num_participants_to_converge, fmt='-og', lw=1)\n",
    "\n",
    "plt.title('Number of Participants Required to Find Optimal Solution')\n",
    "plt.xlabel('Rate of error')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)\n",
    "\n",
    "mean_num_participants_to_solve = np.array(mean_num_participants_to_solve)\n",
    "std_num_participants_to_solve = np.array(std_num_participants_to_solve)\n",
    "max_num_participants_to_solve = np.array(max_num_participants_to_solve)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.errorbar(rate, mean_num_participants_to_solve, std_num_participants_to_solve, fmt='-og', lw=1)\n",
    "\n",
    "plt.title('Number of Participants Required to Find A Solution')\n",
    "plt.xlabel('Rate of error')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hueristic reward for grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass to overwrite the generalized reward:\n",
    "\n",
    "class gridWorld_env1(gridWorld_env):\n",
    "    \n",
    "    def __init__(self, truePositiveRate = 0.7, falsePositiveRate = 0.3):\n",
    "        super(gridWorld_env1,self).__init__(truePositiveRate, falsePositiveRate)\n",
    "        \n",
    "        \n",
    "        # a = 0 is left, a = 1 is down, a = 2 is right, a = 3 is up\n",
    "        #Below defines the hueristic reward. 1 is where we give reward, 0 is not\n",
    "        R = [[0, 1, 1, 0], [0, 1, 1, 0], [0, 1, 1, 0], [1, 1, 1, 0],\n",
    "             [0, 1, 1, 0], [0, 1, 1, 0], [0, 1, 1, 0], [1, 1, 1, 0],\n",
    "             [0, 1, 1, 0], [0, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 1],\n",
    "             [0, 1, 1, 1], [0, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]\n",
    "        \n",
    "        self.R = np.array(R)\n",
    "        \n",
    "        self.k = self.falsePositiveRate*(self.R.size/np.count_nonzero(self.R))\n",
    "        \n",
    "    def getHeuristicReward(self, s,  a, last_s):\n",
    "        if self.R[s, a]:\n",
    "            return self.k\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hueristic Reward results for grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This is baseline performance of our solution\n",
    "\n",
    "num_s = 16\n",
    "num_a = 4\n",
    "\n",
    "optimal_number_steps = 6\n",
    "num_participants = 100\n",
    "num_steps_per_participant = 100\n",
    "\n",
    "num_trial_to_train_initial = 1000\n",
    "\n",
    "\n",
    "#We are going to test where truePositiveRate = 1 - falsePositiveRate\n",
    "rate = np.arange(0.50, 1.01, 0.05)\n",
    "num_times_to_run = 1000\n",
    "\n",
    "num_participants_to_solve_list_ours = []\n",
    "num_participants_to_converge_list_ours = []\n",
    "\n",
    "mean_num_participants_to_converge_ours = []\n",
    "std_num_participants_to_converge_ours = []\n",
    "\n",
    "mean_num_participants_to_solve_ours = []\n",
    "std_num_participants_to_solve_ours = []\n",
    "\n",
    "for r in rate:\n",
    "    env = gridWorld_env1(truePositiveRate = r, falsePositiveRate = 1-r)\n",
    "        \n",
    "    \n",
    "    #We want to find statistics for number of participants required to get optimal_number_steps\n",
    "    #And number of participants needed to simply solve the environment\n",
    "    num_participants_to_converge_list = [];\n",
    "    num_participants_to_solve_list = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_times_to_run):\n",
    "        \n",
    "        #Going to train initial Q0 here\n",
    "        Q0 = np.zeros((num_s, num_a))\n",
    "        for _ in range(num_trial_to_train_initial) :\n",
    "            Q_init = runQLearning(env, 0.02 ,0.95, 1, num_steps_per_participant, Q0, 0,  explore_type = 'constant', explore_const = 1.0)\n",
    "            Q0 = Q_init[-1]  \n",
    "\n",
    "                    \n",
    "        #Run Q learning\n",
    "        Q_saved = runQLearning(env, 0.02,0.95, num_participants, num_steps_per_participant, Q0, 1,  explore_type = 'constant', explore_const = r)\n",
    "        \n",
    "        \n",
    "        steps_all = []\n",
    "        done_all = []\n",
    "\n",
    "        policy = np.zeros((num_s,))\n",
    "        \n",
    "        \n",
    "        #Get the sucess rates\n",
    "        for i in range(len(Q_saved)):\n",
    "            Q = Q_saved[i]    \n",
    "\n",
    "            #Get the policy from that Q\n",
    "            for s in range(num_s):\n",
    "                policy[s] = np.argmax(Q[s,:])\n",
    "                \n",
    "\n",
    "            steps, done = testPolicy(env, policy)\n",
    "\n",
    "            steps_all.append(steps)\n",
    "            done_all.append(done)\n",
    "        \n",
    "        \n",
    "        #Check how many participants were needed to converge\n",
    "        # There is a plus 1 here because the first Q in Q_saved is from Q0 (so 0 participants)\n",
    "        #Also so when it breaks from the loop searching, it will mean we needed the next participant to converge.\n",
    "        num_participants_to_converge = num_participants + 1\n",
    "        for step in steps_all[::-1]:\n",
    "            if step != optimal_number_steps:\n",
    "                break\n",
    "            num_participants_to_converge -= 1\n",
    "\n",
    "        num_participants_to_solve = num_participants + 1\n",
    "        for done in done_all[::-1]:\n",
    "            if not done:\n",
    "                break\n",
    "            num_participants_to_solve -= 1\n",
    "        \n",
    "        num_participants_to_converge_list.append(num_participants_to_converge)\n",
    "        num_participants_to_solve_list.append(num_participants_to_solve)\n",
    "\n",
    "\n",
    "    num_participants_to_converge_list_ours = np.array(num_participants_to_converge_list)\n",
    "        \n",
    "    mean_num_participants_to_converge_ours.append(np.mean(num_participants_to_converge_list))\n",
    "    std_num_participants_to_converge_ours.append(np.std(num_participants_to_converge_list))\n",
    "    \n",
    "    num_participants_to_solve_list_ours = np.array(num_participants_to_solve_list)\n",
    "    \n",
    "    mean_num_participants_to_solve_ours.append(np.mean(num_participants_to_solve_list))\n",
    "    std_num_participants_to_solve_ours.append(np.std(num_participants_to_solve_list))\n",
    "\n",
    "    \n",
    "    print(\"r = \" + str(r))\n",
    "    print(\"mean to converge = \" + str(num_participants_to_converge_list_ours[-1]))\n",
    "    print(\"mean to solve    = \" + str(num_participants_to_solve_list_ours[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline\n",
    "\n",
    "mean_num_participants_to_converge = np.array(mean_num_participants_to_converge)\n",
    "std_num_participants_to_converge = np.array(std_num_participants_to_converge)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.errorbar(rate, mean_num_participants_to_converge, std_num_participants_to_converge, fmt='-og', lw=1)\n",
    "\n",
    "plt.title('Number of Participants Required to Find Optimal Solution')\n",
    "plt.xlabel('Rate of error')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)\n",
    "\n",
    "mean_num_participants_to_solve = np.array(mean_num_participants_to_solve)\n",
    "std_num_participants_to_solve = np.array(std_num_participants_to_solve)\n",
    "max_num_participants_to_solve = np.array(max_num_participants_to_solve)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.errorbar(rate, mean_num_participants_to_solve, std_num_participants_to_solve, fmt='-og', lw=1)\n",
    "\n",
    "plt.title('Number of Participants Required to Find A Solution')\n",
    "plt.xlabel('Rate of error')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)\n",
    "\n",
    "#Plot our solution\n",
    "\n",
    "mean_num_participants_to_converge = np.array(mean_num_participants_to_converge)\n",
    "std_num_participants_to_converge = np.array(std_num_participants_to_converge)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.errorbar(rate, mean_num_participants_to_converge, std_num_participants_to_converge, fmt='-ob', lw=1, label='Baseline')\n",
    "\n",
    "mean_num_participants_to_solve = np.array(mean_num_participants_to_solve)\n",
    "std_num_participants_to_solve = np.array(std_num_participants_to_solve)\n",
    "max_num_participants_to_solve = np.array(max_num_participants_to_solve)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.errorbar(rate, mean_num_participants_to_solve, std_num_participants_to_solve, fmt='-ob', lw=1, label='Baseline')\n",
    "\n",
    "\n",
    "mean_num_participants_to_converge_ours = np.array(mean_num_participants_to_converge_ours)\n",
    "std_num_participants_to_converge_ours = np.array(std_num_participants_to_converge_ours)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.errorbar(rate, mean_num_participants_to_converge_ours, std_num_participants_to_converge_ours, fmt='-og', lw=1, label='Using Heuristic')\n",
    "plt.legend()\n",
    "plt.title('Number of Participants Required to Find Optimal Solution')\n",
    "plt.xlabel('p1 = 1 - p0')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)\n",
    "\n",
    "mean_num_participants_to_solve_ours = np.array(mean_num_participants_to_solve_ours)\n",
    "std_num_participants_to_solve_ours = np.array(std_num_participants_to_solve_ours)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.errorbar(rate, mean_num_participants_to_solve_ours, std_num_participants_to_solve_ours, fmt='-og', lw=1, label='Using Heuristic')\n",
    "plt.legend()\n",
    "plt.title('Number of Participants Required to Find Optimal Solution')\n",
    "plt.xlabel('p1 = 1 - p0')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.ylim(-1, 101)\n",
    "plt.xlim(0.49, 1.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ece276c",
   "language": "python",
   "name": "ece276c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
